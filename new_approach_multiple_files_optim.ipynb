{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "from sklearn.cluster import DBSCAN\n",
    "from torch_geometric.data import Data\n",
    "import torch_geometric.transforms as ttr\n",
    "\n",
    "from astropy.io import fits\n",
    "from astropy.table import Table\n",
    "from astropy import table as astropy_table\n",
    "from astropy import units as u\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy.time import Time\n",
    "\n",
    "from mptd.reader import get_raw_data\n",
    "from mptd.simple_message import SimpleMessage\n",
    "from mptd.plotter import plot_data, plot_clusters, plot_fits_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE=\"cpu\"#\"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "class MPTDData(Data):\n",
    "    \"\"\"\n",
    "    Represents the data for Message-Passing Transient Detection (MPTD).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : None\n",
    "        Feature data (not used in this context).\n",
    "    edge_index : None\n",
    "        Graph edge indices (not used in this context).\n",
    "    edge_attr : None\n",
    "        Graph edge attributes (not used in this context).\n",
    "    y : None\n",
    "        Target labels (not used in this context).\n",
    "    pos : torch.Tensor\n",
    "        Tensor representing the node positions (coordinates).\n",
    "    **kwargs\n",
    "        Additional keyword arguments.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    MPTDData\n",
    "        A new instance of MPTDData with the given node positions.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If the data object has no valid tensor for indexing.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, x=None, edge_index=None, edge_attr=None, y=None, pos=None, **kwargs):\n",
    "        assert x is None\n",
    "        super().__init__(x, edge_index, edge_attr, y, pos, **kwargs)\n",
    "\n",
    "    @property\n",
    "    def x(self):\n",
    "        \"\"\"\n",
    "        Get the node positions (coordinates) stored in 'pos' attribute.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Node positions (coordinates).\n",
    "        \"\"\"\n",
    "        return self.pos\n",
    "    \n",
    "    def append(self, other):\n",
    "        \"\"\"\n",
    "        Append the data from another MPTDData object to the current instance.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        other : MPTDData\n",
    "            Another MPTDData object to append.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        MPTDData\n",
    "            A new instance of MPTDData with the combined data.\n",
    "        \"\"\"\n",
    "        new_edge_index = torch.hstack([self.edge_index, other.edge_index]) \\\n",
    "                         if other.edge_index is not None and self.edge_index is not None else None\n",
    "        new_edge_attr = torch.hstack([self.edge_attr, other.edge_attr]) \\\n",
    "                        if other.edge_attr is not None and self.edge_attr is not None else None\n",
    "        \n",
    "        return MPTDData(x=torch.vstack([self.x, other.x]), \n",
    "                        y=torch.hstack([self.y, other.y]), \n",
    "                        pos=torch.vstack([self.y, other.y]), \n",
    "                        edge_index=new_edge_index,\n",
    "                        edge_attr=new_edge_attr\n",
    "                        )    \n",
    " \n",
    "    def getsplice(self, index):\n",
    "        \"\"\"\n",
    "        Get a new MPTDData object by selecting specific nodes based on the given index.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        index : torch.Tensor\n",
    "            The index tensor used for selecting nodes.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        MPTDData\n",
    "            A new instance of MPTDData with selected nodes.\n",
    "        \"\"\"\n",
    "        y = self.y[index] if self.y is not None else None\n",
    "        pos = self.pos[index] if self.pos is not None else None\n",
    "\n",
    "        return MPTDData(y=y, pos=pos)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Get the number of nodes in the data object.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            Number of nodes in the data object.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            If the data object has no valid tensor for indexing.\n",
    "        \"\"\"\n",
    "        if self.y is not None:\n",
    "            return self.y.size(0)\n",
    "        else:\n",
    "            raise ValueError(\"The data object has no valid tensor for indexing.\")   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MPTDDataset:\n",
    "    \"\"\"\n",
    "    Represents a dataset for Message-Passing Transient Detection (MPTD).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filenames : str or List[str]\n",
    "        File name(s) of the dataset.\n",
    "    keys : List[str]\n",
    "        List of keys used to extract data from the dataset files.\n",
    "    filters : dict\n",
    "        A dictionary containing filters for the dataset.\n",
    "    withsim : bool, optional\n",
    "        Whether to include simulated data, by default True.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filenames, keys, filters:dict, withsim=True) -> None:\n",
    "        if type(filenames) is str:\n",
    "            filenames = [filenames]\n",
    "        filename = filenames[0]\n",
    "        ismos = filename.endswith(\"MIEVLF0000.FTZ\") or filename.endswith(\"MIEVLI0000.FTZ\")\n",
    "        lastcolname = \"PHA\" if ismos else \"TIME_RAW\"\n",
    "        keys_plus = keys + [lastcolname]\n",
    "        raw_data = get_raw_data(filename, keys_plus, filters)\n",
    "        # with fits.open(filename) as hdul:\n",
    "        #     header = hdul[0].header\n",
    "        # frame, date = header[\"RADECSYS\"].lower(), Time(header[\"DATE-OBS\"], format='isot')\n",
    "        # coords = SkyCoord(*(raw_data.to_pandas()[[\"X\",\"Y\"]].to_numpy().T * 0.05), unit=u.arcsec, frame=frame, obstime=date).transform_to(\"icrs\")\n",
    "        # raw_data[\"X\"] = coords.to_table()[\"ra\"]\n",
    "        # raw_data[\"Y\"] = coords.to_table()[\"dec\"]\n",
    "        for filename in filenames[1:]:\n",
    "            ismos = filename.endswith(\"MIEVLF0000.FTZ\") or filename.endswith(\"MIEVLI0000.FTZ\")\n",
    "            lastcolname = \"PHA\" if ismos else \"TIME_RAW\"\n",
    "            keys_plus = keys + [lastcolname]\n",
    "            raw_data = astropy_table.vstack([raw_data, get_raw_data(filename, keys_plus, filters)])\n",
    "            # with fits.open(filename) as hdul:\n",
    "            #     header = hdul[0].header\n",
    "            # frame, date = header[\"RADECSYS\"].lower(), Time(header[\"DATE-OBS\"], format='isot')\n",
    "            # coords = SkyCoord(*(raw_data.to_pandas()[[\"X\",\"Y\"]].to_numpy().T * 0.05), unit=u.arcsec, frame=frame, obstime=date).transform_to(\"icrs\")\n",
    "            # raw_data[\"X\"] = coords.to_table()[\"ra\"]\n",
    "            # raw_data[\"Y\"] = coords.to_table()[\"dec\"]\n",
    "\n",
    "        issimulated = torch.from_numpy(np.array(raw_data[\"ISSIMULATED\"])).bool()\n",
    "        self.groups = torch.from_numpy(np.array(raw_data[lastcolname]))\n",
    "        self.groups[~issimulated] = -1\n",
    "\n",
    "        if withsim:\n",
    "            self.data = MPTDData(pos=torch.from_numpy(np.array([raw_data[key] for key in keys]).T).float(),\n",
    "                                 y=issimulated.long()).to(DEVICE)\n",
    "        else:\n",
    "            self.data = MPTDData(pos=torch.from_numpy(np.array([raw_data[key] for key in keys]).T[~issimulated]).float(),\n",
    "                                 y=issimulated[~issimulated].long()).to(DEVICE)\n",
    "            \n",
    "        self.keys = keys\n",
    "\n",
    "    def get_group(self, group):\n",
    "        \"\"\"\n",
    "        Get nodes belonging to a specific group.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        group : int\n",
    "            Group index.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Node positions (coordinates) belonging to the specified group.\n",
    "        \"\"\"\n",
    "        indices = self.groups == group\n",
    "        return self.data.pos[indices]\n",
    "    \n",
    "    def list_groups(self):\n",
    "        \"\"\"\n",
    "        Get a tensor containing unique group indices.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            A tensor containing unique group indices.\n",
    "        \"\"\"\n",
    "        return torch.unique(self.groups)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MPTDElaborator:\n",
    "    \"\"\"\n",
    "    Performs data elaboration for Message-Passing Transient Detection (MPTD).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : MPTDDataset\n",
    "        The dataset used for elaboration.\n",
    "    transformer : Any\n",
    "        The transformer used to transform the dataset data.\n",
    "    keys : List[str]\n",
    "        List of keys used for data transformation.\n",
    "    model : Any\n",
    "        The model used for data elaboration.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset:MPTDDataset, transformer, keys, model) -> None:\n",
    "        self.dataset = dataset\n",
    "        self.net_data = transformer(dataset.data)\n",
    "        self.keys = keys\n",
    "        self.model = model\n",
    "        self.iterations = 0\n",
    "        self.elaborated_data = torch.ones_like(self.net_data.x[:, 0].unsqueeze(-1))  # Initialize elaborated data\n",
    "\n",
    "    def sizes(self):\n",
    "        \"\"\"\n",
    "        Get the sizes of the elaborated data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Sizes of the elaborated data.\n",
    "        \"\"\"\n",
    "        return self.elaborated_data.squeeze()\n",
    "    \n",
    "    def distances(self):\n",
    "        \"\"\"\n",
    "        Compute the distances between nodes in the elaborated data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Distances between nodes in the elaborated data.\n",
    "        \"\"\"\n",
    "        return torch.norm(self.net_data.pos[self.net_data.edge_index[0]] - self.net_data.pos[self.net_data.edge_index[1]], dim=1)\n",
    "    \n",
    "    def forward(self, iterations=1):\n",
    "        \"\"\"\n",
    "        Perform the forward pass on the model for a specified number of iterations.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        iterations : int, optional\n",
    "            The number of iterations to run the forward pass, by default 1.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Sizes of the elaborated data after the forward pass.\n",
    "        \"\"\"\n",
    "        for _ in range(iterations):\n",
    "            self.elaborated_data += self.model.forward(self.elaborated_data, self.net_data.edge_index)\n",
    "            self.iterations += 1\n",
    "            self.elaborated_data /= self.elaborated_data.max()\n",
    "        return self.sizes()\n",
    "    \n",
    "    def forward_plot(self, iterations, plot_every=1, plot_after=0, max_threshold=0.5):\n",
    "        \"\"\"\n",
    "        Perform forward passes on the model and create plot data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        iterations : int\n",
    "            Total number of iterations to run the forward pass.\n",
    "        plot_every : int, optional\n",
    "            Number of iterations to plot data after, by default 1.\n",
    "        plot_after : int, optional\n",
    "            Number of iterations to run before starting the plot, by default 0.\n",
    "        max_threshold : float, optional\n",
    "            Maximum threshold value, by default 0.5.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Sizes of the elaborated data after the forward pass.\n",
    "        \"\"\"\n",
    "        sizes = self.forward(iterations=plot_after)\n",
    "        threshold = min(sizes.mean().item(), max_threshold)\n",
    "        mask = sizes >= threshold\n",
    "        while self.iterations < iterations:\n",
    "            plot_data(self.net_data.pos[mask].cpu(), sizes[mask].cpu(), issimulated=self.net_data.y[mask].cpu().bool(), \n",
    "                      keys=self.keys, title=f\"iteration {self.iterations}\", \n",
    "                      outfile=osp.join(\"video_frames\", f\"frame_{self.iterations:02}.png\"))\n",
    "            sizes = self.forward(iterations=min(iterations - self.iterations, plot_every))\n",
    "            threshold = min(sizes.mean().item(), max_threshold)\n",
    "            mask = sizes >= threshold\n",
    "        \n",
    "        plot_data(self.net_data.pos[mask].cpu(), sizes[mask].cpu(), issimulated=self.net_data.y[mask].cpu().bool(), \n",
    "                  keys=self.keys, title=f\"iteration {self.iterations}\", \n",
    "                  outfile=osp.join(\"video_frames\", f\"frame_{self.iterations:02}.png\"))\n",
    "\n",
    "        return sizes\n",
    "\n",
    "    def auto_forward(self, rej_threshold, max_threshold):\n",
    "        \"\"\"\n",
    "        Perform an automatic forward pass on the model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        rej_threshold : float\n",
    "            Threshold for rejection.\n",
    "        max_threshold : float\n",
    "            Maximum threshold value.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Sizes of the elaborated data after the forward pass.\n",
    "        \"\"\"\n",
    "        sizes = self.sizes()\n",
    "        threshold = min(sizes.mean().item(), max_threshold)\n",
    "        mask = sizes >= threshold\n",
    "        old_len = mask.sum()\n",
    "        self.elaborated_data += self.model.forward(self.elaborated_data, self.net_data.edge_index)\n",
    "        self.iterations += 1\n",
    "        self.elaborated_data /= self.elaborated_data.max()\n",
    "        sizes = self.sizes()\n",
    "        threshold = min(sizes.mean().item(), max_threshold)\n",
    "        mask = sizes >= threshold\n",
    "        new_len = mask.sum()\n",
    "        while old_len - new_len > rej_threshold:\n",
    "            old_len = new_len\n",
    "            self.elaborated_data += self.model.forward(self.elaborated_data, self.net_data.edge_index)\n",
    "            self.iterations += 1\n",
    "            self.elaborated_data /= self.elaborated_data.max()\n",
    "            sizes = self.sizes()\n",
    "            threshold = min(sizes.mean().item(), max_threshold)\n",
    "            mask = sizes >= threshold\n",
    "            new_len = mask.sum()\n",
    "        return self.sizes()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MPTDClusterer:\n",
    "    \"\"\"\n",
    "    Cluster data using a specified algorithm for Message-Passing Transient Detection (MPTD).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    algorithm : Any\n",
    "        The clustering algorithm to be used.\n",
    "    max_threshold : float\n",
    "        Maximum threshold value.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, algorithm, max_threshold):\n",
    "        self.max_threshold = max_threshold\n",
    "        self.algorithm = algorithm\n",
    "\n",
    "    def mask(self, elaborator:MPTDElaborator):\n",
    "        \"\"\"\n",
    "        Generate a mask for the data using the clustering algorithm.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        elaborator : MPTDElaborator\n",
    "            The MPTDElaborator instance.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            A mask indicating which nodes are part of the cluster.\n",
    "        \"\"\"\n",
    "        sizes = elaborator.sizes()\n",
    "        threshold = min(sizes.mean().item(), self.max_threshold)\n",
    "        mask = sizes >= threshold\n",
    "        return mask\n",
    "\n",
    "    def mask_data(self, elaborator:MPTDElaborator):\n",
    "        \"\"\"\n",
    "        Get the data with nodes masked using the clustering algorithm.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        elaborator : MPTDElaborator\n",
    "            The MPTDElaborator instance.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Masked data with nodes removed.\n",
    "        \"\"\"\n",
    "        mask = self.mask(elaborator)\n",
    "        masked_data = elaborator.net_data.getsplice(mask).cpu()\n",
    "        return masked_data\n",
    "    \n",
    "    def cluster(self, elaborator:MPTDElaborator):\n",
    "        \"\"\"\n",
    "        Cluster the data using the specified algorithm.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        elaborator : MPTDElaborator\n",
    "            The MPTDElaborator instance.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            An array containing cluster labels for each node.\n",
    "        \"\"\"\n",
    "        mask = self.mask(elaborator)\n",
    "        masked_data = elaborator.net_data.getsplice(mask).cpu()\n",
    "        labels = self.algorithm.fit_predict(masked_data.pos)\n",
    "        labels_full = np.full((elaborator.net_data.pos.shape[0],), -1)\n",
    "        labels_full[mask.cpu()] = labels\n",
    "\n",
    "        return labels_full\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "class MPTDScorer:\n",
    "    \"\"\"\n",
    "    Evaluate the performance of the MPTD model using various metrics.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    elaborator : MPTDElaborator\n",
    "        The MPTDElaborator instance.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, elaborator:MPTDElaborator):\n",
    "        self.dataset    = elaborator.dataset\n",
    "        self.elaborator = elaborator\n",
    "        self.labels = None\n",
    "        self.l2g_table = None\n",
    "\n",
    "    def predict_labels(self, clusterer:MPTDClusterer):\n",
    "        \"\"\"\n",
    "        Predict cluster labels using the specified clusterer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        clusterer : MPTDClusterer\n",
    "            The MPTDClusterer instance.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            An array containing cluster labels for each node.\n",
    "        \"\"\"\n",
    "        self.labels = clusterer.cluster(self.elaborator)\n",
    "        self.l2g_table = self.label_to_group_table()\n",
    "        return self.labels\n",
    "\n",
    "    def fluence_vs_success(self):\n",
    "        \"\"\"\n",
    "        Compute the fluence vs. success rate metric.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.tensor\n",
    "            A tensor containing fluence vs. success rate values.\n",
    "        \"\"\"\n",
    "        assert self.labels is not None\n",
    "        result = []\n",
    "        group_table = self.l2g_table.set_index(\"Group\")#.loc[group]\n",
    "        for group in self.dataset.list_groups():\n",
    "            if group < 0: continue\n",
    "            mask = self.dataset.groups == group\n",
    "            assert mask.dtype == torch.bool\n",
    "            fluence = float(mask.sum())\n",
    "            success = not (group_table.loc[group.item()].to_numpy() < 0).all()\n",
    "            result.append([group, fluence, success])\n",
    "        return torch.tensor(result)\n",
    "        return count_and_check_coordinates_grouped(self.dataset.data.pos[self.dataset.data.y.cpu().bool()], self.dataset.groups[self.dataset.data.y.cpu().bool()], self.dataset.data.pos[self.labels >= 0])\n",
    "    \n",
    "    def cluster_accuracy(self):\n",
    "        \"\"\"\n",
    "        Compute the cluster accuracy metric.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The cluster accuracy score.\n",
    "        \"\"\"\n",
    "        assert self.labels is not None\n",
    "        return self.num_true_positives()/len(np.unique(self.labels[self.labels>=0]))\n",
    "        return count_and_check_coordinates_grouped(self.dataset.data.pos[self.labels >= 0], self.labels[self.labels >= 0], self.dataset.data.pos[self.dataset.data.y.bool()]).T[1].float().mean()\n",
    "\n",
    "    def num_true_positives(self):\n",
    "        \"\"\"\n",
    "        Compute the number of true positives.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            The number of true positives.\n",
    "        \"\"\"\n",
    "        assert self.labels is not None\n",
    "        return (self.l2g(np.unique(self.labels[self.labels>=0])) >= 0).sum()\n",
    "        # num = 0\n",
    "        # for label in np.unique(self.labels[self.labels>=0]):\n",
    "        #     mask = self.labels == label\n",
    "        #     num += self.dataset.data.y[mask].any().item()\n",
    "        # return num\n",
    "        # return count_and_check_coordinates_grouped(self.dataset.data.pos[self.labels >= 0], self.labels[self.labels >= 0], self.dataset.data.pos[self.dataset.data.y.bool()]).T[1].sum()\n",
    "\n",
    "    def num_false_positives(self):\n",
    "        \"\"\"\n",
    "        Compute the number of false positives.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            The number of false positives.\n",
    "        \"\"\"\n",
    "        assert self.labels is not None\n",
    "        return (self.l2g(np.unique(self.labels[self.labels>=0])) < 0).sum()\n",
    "        # num = 0\n",
    "        # for label in np.unique(self.labels[self.labels>=0]):\n",
    "        #     mask = self.labels == label\n",
    "        #     num += not self.dataset.data.y[mask].any().item()\n",
    "        # return num\n",
    "        # return (1 - count_and_check_coordinates_grouped(self.dataset.data.pos[self.labels >= 0], self.labels[self.labels >= 0], self.dataset.data.pos[self.dataset.data.y.bool()]).T[1]).sum()\n",
    "    \n",
    "    def label_to_group_table(self):\n",
    "        assert self.labels is not None\n",
    "        result = pd.DataFrame()\n",
    "        for label in np.unique(self.labels[self.labels>=0]):\n",
    "            mask = self.labels == label\n",
    "            masked_groups = self.dataset.groups[mask]\n",
    "            groups = np.unique(masked_groups[masked_groups >= 0])\n",
    "            if len(groups) == 1:\n",
    "                group = groups[0]\n",
    "            else:\n",
    "                group = -1\n",
    "            new_row = pd.DataFrame({\"Label\": label, \"Group\": group}, index=[0])\n",
    "            result = pd.concat([result, new_row], ignore_index=True)\n",
    "        unlabeled_groups = np.setdiff1d(self.dataset.list_groups(), np.unique(result[\"Group\"].to_numpy()), assume_unique=True)\n",
    "        if len(unlabeled_groups) > 0:\n",
    "            leftovers = pd.DataFrame({\"Group\": unlabeled_groups,\n",
    "                                    \"Label\": np.full_like(unlabeled_groups, -1)})\n",
    "            result = pd.concat([result, leftovers], ignore_index=True)\n",
    "        return result\n",
    "    \n",
    "    def l2g(self, label):\n",
    "        assert self.l2g_table is not None\n",
    "        return self.l2g_table.set_index(\"Label\").loc[label, \"Group\"]\n",
    "    \n",
    "    def g2l(self, group):\n",
    "        assert self.l2g_table is not None\n",
    "        return self.l2g_table.set_index(\"Group\").loc[group, \"Label\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_common_parts(strings):\n",
    "    \"\"\"\n",
    "    Find the common characters at each position among a list of strings.\n",
    "\n",
    "    Given a list of strings, this function returns a new string that contains\n",
    "    the characters that appear at the same position in all the input strings.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    strings : List[str]\n",
    "        A list of strings for which the common characters at each position need to be found.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        A new string containing the common characters at each position among the input strings.\n",
    "        If the input list is empty or the strings have different lengths, an empty string is returned.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> find_common_parts([\"apple\", \"apricot\", \"apartment\"])\n",
    "    'ap'\n",
    "    >>> find_common_parts([\"cat\", \"bat\", \"rat\"])\n",
    "    'a'\n",
    "    >>> find_common_parts([\"apple\", \"apricot\", \"banana\"])\n",
    "    ''\n",
    "    \"\"\"\n",
    "    if not strings:\n",
    "        return \"\"\n",
    "\n",
    "    common_parts = \"\"\n",
    "    for chars_at_position in zip(*strings):\n",
    "        if all(char == chars_at_position[0] for char in chars_at_position):\n",
    "            common_parts += chars_at_position[0]\n",
    "\n",
    "    return common_parts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_file_scoring(filenames, keys, k, rej_threshold, max_threshold, filters, min_samples, withsim=True, max_time_interval=np.inf):\n",
    "    dataset = MPTDDataset(filenames, keys, filters, withsim=withsim)\n",
    "    transformer = ttr.KNNGraph(k=k, force_undirected=True)\n",
    "    model = SimpleMessage()\n",
    "    elaborator = MPTDElaborator(dataset, transformer, keys, model)\n",
    "\n",
    "    eps = elaborator.distances().max().item()\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    clusterer = MPTDClusterer(algorithm=dbscan, max_threshold=max_threshold)\n",
    "    scorer = MPTDScorer(elaborator)\n",
    "\n",
    "    if save_every is None:\n",
    "        save_every = np.inf\n",
    "\n",
    "    groups = pd.DataFrame()\n",
    "    start_time = time.time()\n",
    "\n",
    "    scorer.elaborator.auto_forward(rej_threshold, max_threshold)\n",
    "    labels = scorer.predict_labels(clusterer)\n",
    "\n",
    "    for label in np.unique(scorer.labels):\n",
    "        if label < 0:\n",
    "            continue\n",
    "        mask = scorer.labels == label\n",
    "        group_times = scorer.elaborator.dataset.data.pos[mask, 1].cpu()\n",
    "        time_interval = (group_times.max() - group_times.min()).item()\n",
    "        if time_interval > max_time_interval:\n",
    "            scorer.labels[mask] = -1\n",
    "            continue\n",
    "        group_xs = scorer.elaborator.dataset.data.pos[mask, 0].cpu()\n",
    "        group_ys = scorer.elaborator.dataset.data.pos[mask, 2].cpu()\n",
    "        det_grp = scorer.l2g(label)\n",
    "        # det_grp = scorer.l2g(label).to_numpy()\n",
    "        # assert len(det_grp) == 1\n",
    "        # det_grp = det_grp[0]\n",
    "        det_grp_fl = len(scorer.elaborator.dataset.groups[mask][scorer.elaborator.dataset.groups[mask] == det_grp]) if det_grp >= 0 else 0\n",
    "        new_row = pd.DataFrame({\"Mean Time\": group_times.mean().item(),\n",
    "                                \"Duration\": time_interval, \n",
    "                                \"Mean X\": group_xs.mean().item(),\n",
    "                                \"Std X\": group_xs.std().item(),\n",
    "                                \"Mean Y\": group_ys.mean().item(),\n",
    "                                \"Std Y\": group_ys.mean().item(),\n",
    "                                \"Counts\": len(group_times),\n",
    "                                \"File\": str(filenames),\n",
    "                                \"Group ID\": label,\n",
    "                                \"Rejection\": rej_threshold,\n",
    "                                \"Iteration\": scorer.elaborator.iterations,\n",
    "                                \"Detects\": det_grp >= 0, #scorer.elaborator.dataset.data.y[mask].any().item(),\n",
    "                                \"Detected Group\": det_grp,\n",
    "                                \"Detected Group Fluence\": det_grp_fl,\n",
    "                                \"Original Group Fluence\": len(scorer.elaborator.dataset.get_group(det_grp)) if det_grp >= 0 else 0\n",
    "                                }, index=[0])\n",
    "        groups = pd.concat([groups, new_row], ignore_index=True)\n",
    "\n",
    "    results = pd.DataFrame(scorer.fluence_vs_success().float(), columns=[\"ID\", \"Fluence\", \"Detected\"])\n",
    "    results[\"File\"] = np.full(results[\"Fluence\"].shape, filenames)\n",
    "    results[\"Accuracy\"] = np.full_like(results[\"Fluence\"], scorer.cluster_accuracy())\n",
    "    results[\"Rejection\"] = np.full_like(results[\"Fluence\"], rej_threshold)\n",
    "    results[\"Iteration\"] = np.full_like(results[\"Fluence\"], scorer.elaborator.iterations)\n",
    "    results[\"True Pos\"] = np.full_like(results[\"Fluence\"], scorer.num_true_positives())\n",
    "    results[\"False Pos\"] = np.full_like(results[\"Fluence\"], scorer.num_false_positives())\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    results[\"Time\"] = elapsed_time\n",
    "\n",
    "    return results, groups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, keys, ks, rej_thresholds, max_thresholds, filters, min_samples_range, withsim, max_time_intervals):\n",
    "    k = trial.suggest_int(\"k\", *ks)\n",
    "    rej_threshold = trial.suggest_int(\"rej_threshold\", *rej_threshols)\n",
    "    min_samples = trial.suggest_int(\"min_samples\", *min_samples_range)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
